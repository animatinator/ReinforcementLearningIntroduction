# REINFORCE: Monte-Carlo policy gradient control (episodic).

from corridor_env import Action, ShortCorridor, TimeStep
from dataclasses import dataclass
import math
import matplotlib.pyplot as plt
import numpy as np


ALPHA = 0.02
ALPHA_STATELESS = 0.0002
DISCOUNT = 0.9
NUM_TRAIN_EPISODES = 2000
NUM_TRAIN_EPISODES_STATELESS = 10000


# A parameterised policy that evaluates states and actions.
class SimpleParameterisedPolicy:
	def __init__(self, num_states, action_type):
		self._action_type = action_type
		self._weights = np.zeros((num_states, len(self._action_type)))
	
	def _compute_preference(self, state, action):
		return self._weights[state, action.value]

	# Evaluate the relative preferences of each action. This uses an
	# exponential softmax.
	def evaluate_actions(self, state):
		preferences = np.zeros((len(self._action_type)))
		total_preference = 0.0

		for action in self._action_type:
			preference_exp = math.exp(self._compute_preference(state, action))
			total_preference += preference_exp
			preferences[action.value] = preference_exp

		if total_preference == 0.0:
			return np.ones((len(self._action_type))) / len(self._action_type)
		return preferences / total_preference

	# Select an action according to the probability distribution generated by
	# evaluate_actions.
	def select_action(self, state):
		probabilities = self.evaluate_actions(state)
		action_id = np.random.choice([i for i in range(0, len(self._action_type))], p=probabilities)
		return Action(action_id)

	def adjust_weights_along_gradient(self, state, action, scale):
		gradient = np.zeros(len(self._action_type))
		gradient[action.value] = 1

		action_probabilities = self.evaluate_actions(state)
		for action in self._action_type:
			gradient[action.value] -= action_probabilities[action.value]

		self._weights[state, :] += gradient * scale


# A parameterised policy that only considers actions and ignores states.
class ActionOnlyParameterisedPolicy:
	def __init__(self, action_type):
		self._action_type = action_type
		self._weights = np.zeros((len(self._action_type)))

	def _compute_preference(self, state, action):
		return self._weights[action.value]

	# Evaluate the relative preferences of each action. This uses an
	# exponential softmax.
	def evaluate_actions(self, state):
		preferences = np.zeros((len(self._action_type)))
		total_preference = 0.0

		for action in self._action_type:
			preference_exp = math.exp(self._compute_preference(state, action))
			total_preference += preference_exp
			preferences[action.value] = preference_exp

		if total_preference == 0.0:
			return np.ones((len(self._action_type))) / len(self._action_type)
		return preferences / total_preference

	# Select an action according to the probability distribution generated by
	# evaluate_actions.
	def select_action(self, state):
		probabilities = self.evaluate_actions(state)
		action_id = np.random.choice([i for i in range(0, len(self._action_type))], p=probabilities)
		return Action(action_id)

	def adjust_weights_along_gradient(self, state, action, scale):
		gradient = np.zeros(len(self._action_type))
		gradient[action.value] = 1

		action_probabilities = self.evaluate_actions(state)
		for action in self._action_type:
			gradient[action.value] -= action_probabilities[action.value]

		self._weights += gradient * scale


@dataclass
class EpisodeStep:
	step: TimeStep
	action: Action


# Generate an episode by applying the policy to the environment.
def generate_episode(env, policy):
	step = env.reset()
	episode = []

	while not step.terminal:
		action = policy.select_action(step.state)
		episode.append(EpisodeStep(step, action))
		step = env.step(step.state, action)
	episode.append(EpisodeStep(step, None))

	return episode

# Adjust the policy based on a real episode.
def learn_from_episode(policy, episode, alpha):
	# The last step doesn't take an action so we can't learn from it.
	for step in range(0, len(episode) - 1):
		G = 0
		for substep in range(step + 1, len(episode)):
			G += math.pow(DISCOUNT, substep - (step + 1)) * episode[substep].step.reward
		policy.adjust_weights_along_gradient(
				episode[step].step.state, episode[step].action, alpha * math.pow(DISCOUNT, step) * G)

# Train the policy on an environment by repeatedly generating episodes and
# learning from them.
# Returns a list of episode lengths.
def train_policy(policy, env, alpha, num_episodes):
	episode_lengths = []

	for episode_id in range(0, num_episodes):
		episode = generate_episode(env, policy)
		episode_lengths.append(len(episode))

		learn_from_episode(policy, episode, alpha)

	return episode_lengths

def graph_training_process(env, policy, alpha=ALPHA, num_episodes=NUM_TRAIN_EPISODES):
	episode_lengths = train_policy(policy, env, alpha, num_episodes)
	xs = [x for x in range(0, len(episode_lengths))]
	plt.plot(xs, episode_lengths)
	plt.show()


if __name__ == '__main__':
	env = ShortCorridor()
	policy = SimpleParameterisedPolicy(4, Action)
	graph_training_process(env, policy)
	policy = ActionOnlyParameterisedPolicy(Action)
	graph_training_process(env, policy, ALPHA_STATELESS, NUM_TRAIN_EPISODES_STATELESS)
