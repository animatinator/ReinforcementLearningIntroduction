# REINFORCE: Monte-Carlo policy gradient control (episodic).

from corridor_env import Action, ShortCorridor
import numpy as np


ALPHA = 0.000000000002  # TODO(DO NOT SUBMIT): Really?
NUM_TRAIN_STEPS = 10000


class SimpleParameterisedPolicy:
	def __init__(self, num_states, action_type):
		self._action_type = action_type
		self._weights = np.zeros((num_states, len(self._action_type)))
	
	def _compute_preference(self, state, action):
		return self._weights[state, action.value]

	# Evaluate the relative preferences of each action. This uses an
	# exponential softmax.
	def evaluate_actions(self, state):
		preferences = np.zeros((len(self._action_type)))
		total_preference = 0.0

		for action in self._action_type:
			preference = self._compute_preference(state, action)
			total_preference += preference
			preferences[action.value] = preference

		if total_preference == 0.0:
			return np.ones((len(self._action_type))) / len(self._action_type)
		return preferences / total_preference

	# Select an action according to the probability distribution generated by
	# evaluate_actions.
	def select_action(self, state):
		probabilities = self.evaluate_actions(state)
		action_id = np.random.choice([i for i in range(0, len(self._action_type))], p=probabilities)
		return Action(action_id)


# Generate an episode by applying the policy to the environment.
def generate_episode(env, policy):
	step = env.reset()
	episode = [step]

	while not step.terminal:
		action = policy.select_action(step.state)
		step = env.step(step.state, action)
		episode.append(step)

	return episode


if __name__ == '__main__':
	env = ShortCorridor()
	policy = SimpleParameterisedPolicy(4, Action)

	state = env.reset()
	print(generate_episode(env, policy))
