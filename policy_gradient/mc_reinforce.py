# REINFORCE: Monte-Carlo policy gradient control (episodic).

from corridor_env import Action, ShortCorridor
import numpy as np


ALPHA = 0.000000000002  # TODO(DO NOT SUBMIT): Really?
NUM_TRAIN_STEPS = 10000


class SimpleParameterisedPolicy:
	def __init__(self, num_states, action_type):
		self._action_type = action_type
		self._weights = np.zeros((num_states, len(action_type)))
	
	def _compute_preference(self, state, action):
		return self._weights[state, action.value]

	# Evaluate the relative preferences of each action. This uses an
	# exponential softmax.
	def evaluate_actions(self, state):
		preferences = np.zeros((len(action_type)))
		total_preference = 0.0

		for action in self._action_type:
			preference = self._compute_preference(state, action)
			total_preference += preference
			preferences[action] = preferences

		return preferences / total_preference

	# Select an action according to the probability distribution generated by
	# evaluate_actions.
	def select_action(self, state):
		probabilities = self.evaluate_actions(state)
		return np.random.choice([i for i in range(0, len(self._action_type))], probabilities)


if __name__ == '__main__':
	env = ShortCorridor()
	policy = SimpleParameterisedPolicy(4, Action)

	state = env.reset()
